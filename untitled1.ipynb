{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "V5E1",
      "runtime_attributes": {
        "runtime_version": "2025.10"
      },
      "authorship_tag": "ABX9TyNjp+oHDdhTOR88GECicJs6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FuzzysTodd/Foundry-Fund-Me-2024/blob/main/untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('secretName')"
      ],
      "metadata": {
        "id": "kb36QYiDRBSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3qLtZH-QId0"
      },
      "source": [
        "# Import the Python SDK\n",
        "import google.generativeai as genai\n",
        "# Used to securely store your API key\n",
        "from google.colab import userdata\n",
        "\n",
        "# Replace 'YOUR_SECRET_NAME' with the name you used in the Secrets Manager\n",
        "GOOGLE_API_KEY=userdata.get('YOUR_SECRET_NAME')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2qkFv0K5QBw"
      },
      "outputs": [],
      "source": [
        "# Superalgos AI-Blockchain Integration System\n",
        "\n",
        "## Overview\n",
        "\n",
        "This system integrates AI deep think processes with blockchain ledger technology, implementing a mathematical map in a DAO platform using computational 3Algebra PHI and organic chemistry tracking.\n",
        "\n",
        "## Features\n",
        "\n",
        "### 1. AI Integration System\n",
        "- **Deep Think Processing**: Processes AI deep think completion events\n",
        "- **Multi-AI Coordination**: Involves all knowable AIs in query processing\n",
        "- **Constant Movement Tracking**: Tracks each AI idea in constant mathematical movement\n",
        "\n",
        "### 2. Blockchain Ledger\n",
        "- **Data Forms & Datagrams**: Creates structured data forms and datagrams for blockchain\n",
        "- **AI Idea Ledgering**: Records each AI idea to the blockchain with immutable history\n",
        "- **Verification**: Maintains blockchain integrity with hash verification\n",
        "\n",
        "### 3. Math Map DAO\n",
        "- **Mathematical Mapping**: Runs a mathematical map in the DAO platform\n",
        "- **3D Node Network**: Tracks AI ideas as nodes in 3-dimensional space\n",
        "- **Constant Movement**: Monitors and calculates movement of mathematical ideas\n",
        "- **DAO Governance**: Implements proposal and voting mechanisms\n",
        "\n",
        "### 4. Computational 3Algebra PHI\n",
        "- **Golden Ratio Computing**: Uses PHI (1.618...) for mathematical transformations\n",
        "- **3-Dimensional Algebra**: Operates in 3D algebraic space\n",
        "- **PHI Spiral Calculations**: Generates PHI-based spirals and geometric structures\n",
        "- **Fibonacci Lattices**: Creates Fibonacci lattice points for spatial organization\n",
        "\n",
        "### 5. Organic Chemistry Tracker\n",
        "- **Element Tracking**: Tracks total organic elements (C, H, O, N, S, P)\n",
        "- **Chemical Reactions**: Records chemical reactions derived from PHI computations\n",
        "- **PHI-Based Structures**: Analyzes golden ratio relationships in molecular structures\n",
        "- **Molecular Weight Calculations**: Computes molecular weights and compositions\n",
        "\n",
        "## Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BTcefX_2PWBS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69998c47"
      },
      "source": [
        "To use the Gemini API, you'll need an API key. If you don't already have one, create a key in Google AI Studio.\n",
        "In Colab, add the key to the secrets manager under the \"ðŸ”‘\" in the left panel. Give it a name, for example, `GOOGLE_API_KEY` or `GEMINI_API_KEY_1` as you mentioned. Then pass the key to the SDK:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cec41603"
      },
      "source": [
        "# Import the Python SDK\n",
        "import google.generativeai as genai\n",
        "# Used to securely store your API key\n",
        "from google.colab import userdata\n",
        "\n",
        "# Replace 'YOUR_SECRET_NAME' with the name you used in the Secrets Manager\n",
        "GOOGLE_API_KEY=userdata.get('YOUR_SECRET_NAME')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4154755e"
      },
      "source": [
        "!pip install google-generativeai"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5459e591"
      },
      "source": [
        "Before you can make any API calls, you need to initialize the Generative Model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b375f2ed"
      },
      "source": [
        "# Initialize the Gemini API\n",
        "# You can specify a model name here, like 'gemini-pro' or other available models\n",
        "gemini_model = genai.GenerativeModel('gemini-pro')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "827fad5f"
      },
      "source": [
        "Now you can make API calls. For example, to generate text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac779508"
      },
      "source": [
        "prompt = \"Write a short poem about the future of AI.\"\n",
        "response = gemini_model.generate_content(prompt)\n",
        "print(response.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('secretName')"
      ],
      "metadata": {
        "id": "kgiaxX-aPSnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "bYXJi0npLmRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# https://cloud.google.com/resource-manager/docs/creating-managing-projects\n",
        "project_id = '[your Cloud Platform project ID]'\n",
        "sample_count = 2000\n",
        "\n",
        "row_count = pd.io.gbq.read_gbq('''\n",
        "  SELECT\n",
        "    COUNT(*) as total\n",
        "  FROM `bigquery-public-data.samples.gsod`\n",
        "''', project_id=project_id).total[0]\n",
        "\n",
        "df = pd.io.gbq.read_gbq(f'''\n",
        "  SELECT\n",
        "    *\n",
        "  FROM\n",
        "    `bigquery-public-data.samples.gsod`\n",
        "  WHERE RAND() < {sample_count}/{row_count}\n",
        "''', project_id=project_id)\n",
        "\n",
        "print(f'Full dataset has {row_count} rows')"
      ],
      "metadata": {
        "id": "gTPHNBSELmRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0748bbec"
      },
      "source": [
        "# Superalgos AI-Blockchain Integration System\n",
        "\n",
        "## Overview\n",
        "\n",
        "This system integrates AI deep think processes with blockchain ledger technology, implementing a mathematical map in a DAO platform using computational 3Algebra PHI and organic chemistry tracking.\n",
        "\n",
        "## Features\n",
        "\n",
        "### 1. AI Integration System\n",
        "- **Deep Think Processing**: Processes AI deep think completion events\n",
        "- **Multi-AI Coordination**: Involves all knowable AIs in query processing\n",
        "- **Constant Movement Tracking**: Tracks each AI idea in constant mathematical movement\n",
        "\n",
        "### 2. Blockchain Ledger\n",
        "- **Data Forms & Datagrams**: Creates structured data forms and datagrams for blockchain\n",
        "- **AI Idea Ledgering**: Records each AI idea to the blockchain with immutable history\n",
        "- **Verification**: Maintains blockchain integrity with hash verification\n",
        "\n",
        "### 3. Math Map DAO\n",
        "- **Mathematical Mapping**: Runs a mathematical map in the DAO platform\n",
        "- **3D Node Network**: Tracks AI ideas as nodes in 3-dimensional space\n",
        "- **Constant Movement**: Monitors and calculates movement of mathematical ideas\n",
        "- **DAO Governance**: Implements proposal and voting mechanisms\n",
        "\n",
        "### 4. Computational 3Algebra PHI\n",
        "- **Golden Ratio Computing**: Uses PHI (1.618...) for mathematical transformations\n",
        "- **3-Dimensional Algebra**: Operates in 3D algebraic space\n",
        "- **PHI Spiral Calculations**: Generates PHI-based spirals and geometric structures\n",
        "- **Fibonacci Lattices**: Creates Fibonacci lattice points for spatial organization\n",
        "\n",
        "### 5. Organic Chemistry Tracker\n",
        "- **Element Tracking**: Tracks total organic elements (C, H, O, N, S, P)\n",
        "- **Chemical Reactions**: Records chemical reactions derived from PHI computations\n",
        "- **PHI-Based Structures**: Analyzes golden ratio relationships in molecular structures\n",
        "- **Molecular Weight Calculations**: Computes molecular weights and compositions\n",
        "\n",
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "mTpj9t81LmRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2780cafe"
      },
      "source": [
        "# Re-execute the cell to load data from BigQuery\n",
        "import pandas as pd\n",
        "\n",
        "# https://cloud.google.com/resource-manager/docs/creating-managing-projects\n",
        "project_id = '[your Cloud Platform project ID]'\n",
        "sample_count = 2000\n",
        "\n",
        "row_count = pd.io.gbq.read_gbq('''\n",
        "  SELECT\n",
        "    COUNT(*) as total\n",
        "  FROM `bigquery-public-data.samples.gsod`\n",
        "''', project_id=project_id).total[0]\n",
        "\n",
        "df = pd.io.gbq.read_gbq(f'''\n",
        "  SELECT\n",
        "    *\n",
        "  FROM\n",
        "    `bigquery-public-data.samples.gsod`\n",
        "  WHERE RAND() < {sample_count}/{row_count}\n",
        "''', project_id=project_id)\n",
        "\n",
        "print(f'Full dataset has {row_count} rows')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74de3e11"
      },
      "source": [
        "# Re-execute the cell to load data from BigQuery\n",
        "import pandas as pd\n",
        "\n",
        "# https://cloud.google.com/resource-manager/docs/creating-managing-projects\n",
        "project_id = 'REPLACE WITH YOUR GOOGLE CLOUD PROJECT ID' # REPLACE WITH YOUR GOOGLE CLOUD PROJECT ID\n",
        "sample_count = 2000\n",
        "\n",
        "row_count = pd.io.gbq.read_gbq('''\n",
        "  SELECT\n",
        "    COUNT(*) as total\n",
        "  FROM `bigquery-public-data.samples.gsod`\n",
        "''', project_id=project_id).total[0]\n",
        "\n",
        "df = pd.io.gbq.read_gbq(f'''\n",
        "  SELECT\n",
        "    *\n",
        "  FROM\n",
        "    `bigquery-public-data.samples.gsod`\n",
        "  WHERE RAND() < {sample_count}/{row_count}\n",
        "''', project_id=project_id)\n",
        "\n",
        "print(f'Full dataset has {row_count} rows')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cb2eceba"
      },
      "source": [
        "# Re-execute the cell to describe the dataframe\n",
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeaa1da5"
      },
      "source": [
        "To set up BigQuery credentials, you need to authenticate with Google Cloud Platform. You can do this in Colab using the `google.colab.auth` library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c459fee8"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffbe5761"
      },
      "source": [
        "After running the cell above, you will be prompted to authenticate and authorize access to your Google Cloud account. Follow the instructions in the pop-up window."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd79a0fd"
      },
      "source": [
        "# Re-execute the cell to load data from BigQuery\n",
        "import pandas as pd\n",
        "\n",
        "# https://cloud.google.com/resource-manager/docs/creating-managing-projects\n",
        "project_id = '[your Cloud Platform project ID]'\n",
        "sample_count = 2000\n",
        "\n",
        "row_count = pd.io.gbq.read_gbq('''\n",
        "  SELECT\n",
        "    COUNT(*) as total\n",
        "  FROM `bigquery-public-data.samples.gsod`\n",
        "''', project_id=project_id).total[0]\n",
        "\n",
        "df = pd.io.gbq.read_gbq(f'''\n",
        "  SELECT\n",
        "    *\n",
        "  FROM\n",
        "    `bigquery-public-data.samples.gsod`\n",
        "  WHERE RAND() < {sample_count}/{row_count}\n",
        "''', project_id=project_id)\n",
        "\n",
        "print(f'Full dataset has {row_count} rows')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd8d477f"
      },
      "source": [
        "# Re-execute the cell to describe the dataframe\n",
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d4023a1"
      },
      "source": [
        "import sys\n",
        "print(f\"Python version: {sys.version}\")\n",
        "import platform\n",
        "print(f\"Platform: {platform.platform()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add `%load_ext cudf.pandas` before importing pandas to speed up operations using GPU"
      ],
      "metadata": {
        "id": "m0-xeBxPMBbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext cudf.pandas\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define the number of rows\n",
        "num_rows = 1000000\n",
        "\n",
        "states = [\"NY\", \"NJ\", \"CA\", \"TX\"]\n",
        "violations = [\"Double Parking\", \"Expired Meter\", \"No Parking\", \"Fire Hydrant\",\n",
        "              \"Bus Stop\"]\n",
        "vehicle_types = [\"SUBN\", \"SDN\"]\n",
        "\n",
        "# Generate random data for Dataset 1\n",
        "data1 = {\n",
        "    \"Registration State\": np.random.choice(states, size=num_rows),\n",
        "    \"Ticket Number\": np.random.randint(1000000000, 9999999999, size=num_rows)\n",
        "}\n",
        "\n",
        "# Generate random data for Dataset 2\n",
        "data2 = {\n",
        "    \"Ticket Number\": np.random.choice(data1['Ticket Number'], size=num_rows),  # Reusing ticket numbers to ensure matches\n",
        "    \"Violation Description\": np.random.choice(violations, size=num_rows)\n",
        "}\n",
        "\n",
        "# Create DataFrames\n",
        "df1 = pd.DataFrame(data1)\n",
        "df2 = pd.DataFrame(data2)\n",
        "\n",
        "# Perform an inner join on 'Ticket Number'\n",
        "merged_df = pd.merge(df1, df2, on=\"Ticket Number\", how=\"inner\")\n",
        "\n",
        "# Display some of the joined data\n",
        "print(merged_df.head())"
      ],
      "metadata": {
        "id": "G8XCQ9IBMBbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e630efa"
      },
      "source": [
        "println(\"Hello from Julia!\")\n",
        "\n",
        "# Define a variable\n",
        "x = 10\n",
        "\n",
        "# Perform a calculation\n",
        "y = x * 2 + 5\n",
        "\n",
        "println(\"The value of y is: \", y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "from google.colab import output\n",
        "\n",
        "def Concat(a, b):\n",
        "  # Use display.JSON to transfer a structured result.\n",
        "  return IPython.display.JSON({'result': ' '.join((a, b))})\n",
        "\n",
        "output.register_callback('notebook.Concat', Concat)"
      ],
      "metadata": {
        "id": "wpc4UdgVMRAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf05832e"
      },
      "source": [
        "# Summarize the merged_df DataFrame\n",
        "print(\"Summary of merged_df:\")\n",
        "display(merged_df.describe())\n",
        "\n",
        "print(\"\\nInformation about merged_df:\")\n",
        "merged_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%javascript\n",
        "(async function() {\n",
        "  const result = await google.colab.kernel.invokeFunction(\n",
        "    'notebook.Concat', // The callback name.\n",
        "    ['hello', 'world!'], // The arguments.\n",
        "    {}); // kwargs\n",
        "  const text = result.data['application/json'];\n",
        "  document.querySelector(\"#output-area\").appendChild(document.createTextNode(text.result));\n",
        "})();"
      ],
      "metadata": {
        "id": "nlOCf4otMRAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "from google.colab import output\n",
        "\n",
        "display(IPython.display.HTML('''\n",
        "    The items:\n",
        "    <br><ol id=\"items\"></ol>\n",
        "    <button id='button'>Click to add</button>\n",
        "    <script>\n",
        "      document.querySelector('#button').onclick = () => {\n",
        "        google.colab.kernel.invokeFunction('notebook.AddListItem', [], {});\n",
        "      };\n",
        "    </script>\n",
        "    '''))\n",
        "\n",
        "def add_list_item():\n",
        "  # Use redirect_to_element to direct the elements which are being written.\n",
        "  with output.redirect_to_element('#items'):\n",
        "    # Use display to add items which will be persisted on notebook reload.\n",
        "    display(IPython.display.HTML('<li> Another item</li>'))\n",
        "\n",
        "output.register_callback('notebook.AddListItem', add_list_item)"
      ],
      "metadata": {
        "id": "Tv_DIY5jMRAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "import uuid\n",
        "from google.colab import output\n",
        "\n",
        "class InvokeButton(object):\n",
        "  def __init__(self, title, callback):\n",
        "    self._title = title\n",
        "    self._callback = callback\n",
        "\n",
        "  def _repr_html_(self):\n",
        "    callback_id = 'button-' + str(uuid.uuid4())\n",
        "    output.register_callback(callback_id, self._callback)\n",
        "\n",
        "    template = \"\"\"<button id=\"{callback_id}\">{title}</button>\n",
        "        <script>\n",
        "          document.querySelector(\"#{callback_id}\").onclick = (e) => {{\n",
        "            google.colab.kernel.invokeFunction('{callback_id}', [], {{}})\n",
        "            e.preventDefault();\n",
        "          }};\n",
        "        </script>\"\"\"\n",
        "    html = template.format(title=self._title, callback_id=callback_id)\n",
        "    return html\n",
        "\n",
        "def do_something():\n",
        "  print('here')\n",
        "\n",
        "InvokeButton('click me', do_something)"
      ],
      "metadata": {
        "id": "z9FGquDsMRAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07ffa2e3"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# 1. Define the Golden Ratio (PHI)\n",
        "PHI = (1 + np.sqrt(5)) / 2\n",
        "print(f\"The Golden Ratio (PHI) is approximately: {PHI}\")\n",
        "\n",
        "# 2. Basic calculations with PHI\n",
        "print(f\"PHI squared: {PHI**2}\")\n",
        "print(f\"1 / PHI: {1 / PHI}\")\n",
        "print(f\"PHI - 1: {PHI - 1}\")\n",
        "# Note that PHI**2 is approximately PHI + 1, and 1/PHI is approximately PHI - 1\n",
        "\n",
        "# 3. Representing points in 3D space\n",
        "# A simple 3D point can be represented as a tuple or list\n",
        "point_a = (1, 2, 3)\n",
        "point_b = [PHI, PHI**2, PHI + 1]\n",
        "print(f\"\\nPoint A in 3D: {point_a}\")\n",
        "print(f\"Point B in 3D (using PHI): {point_b}\")\n",
        "\n",
        "# You can use NumPy arrays for more complex 3D vector operations\n",
        "vector_a = np.array(point_a)\n",
        "vector_b = np.array(point_b)\n",
        "print(f\"Vector A (NumPy array): {vector_a}\")\n",
        "print(f\"Vector B (NumPy array): {vector_b}\")\n",
        "\n",
        "# Example vector addition\n",
        "vector_c = vector_a + vector_b\n",
        "print(f\"Vector A + Vector B: {vector_c}\")\n",
        "\n",
        "# 4. Conceptualizing a PHI-based spiral in 3D\n",
        "# A 2D Golden Spiral can be generated using polar coordinates with the formula r = a * exp(b * theta)\n",
        "# For a Golden Spiral, b is related to PHI by b = (ln(PHI)) / (pi / 2)\n",
        "# To extend this to 3D, you might add a z-component that also changes with theta or distance from origin\n",
        "\n",
        "def golden_spiral_3d_points(num_points=100, a=1,  max_theta=10 * np.pi):\n",
        "    \"\"\"Generates points along a conceptual 3D Golden Spiral.\"\"\"\n",
        "    b = np.log(PHI) / (np.pi / 2)\n",
        "    theta = np.linspace(0, max_theta, num_points)\n",
        "    r = a * np.exp(b * theta)\n",
        "\n",
        "    x = r * np.cos(theta)\n",
        "    y = r * np.sin(theta)\n",
        "    # For a simple 3D extension, let's make z proportional to theta\n",
        "    z = theta * 0.5 # Adjust the multiplier to change the spiral's height\n",
        "\n",
        "    return x, y, z\n",
        "\n",
        "# Generate points\n",
        "x_spiral, y_spiral, z_spiral = golden_spiral_3d_points()\n",
        "\n",
        "# Visualize the 3D spiral (requires matplotlib with 3D plotting enabled)\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot(x_spiral, y_spiral, z_spiral)\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.set_zlabel('Z')\n",
        "ax.set_title('Conceptual 3D Golden Spiral')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "id": "t1EuTTArLNLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/gdrive/My Drive/foo.txt', 'w') as f:\n",
        "  f.write('Hello Google Drive!')\n",
        "!cat '/gdrive/My Drive/foo.txt'"
      ],
      "metadata": {
        "id": "bARaRnwVLNLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "692f0b31"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "# --- Create a hypothetical target variable for demonstration ---\n",
        "# In a real scenario, your target variable would come from your dataset\n",
        "# For this example, let's create a binary target based on some arbitrary condition\n",
        "# (e.g., whether the original data string contains the word 'the')\n",
        "# This is just for demonstration and won't be meaningful for real AI learning\n",
        "# You would replace this with your actual target variable\n",
        "if 'ai_learning_data' in locals() and isinstance(ai_learning_data, pd.DataFrame) and 'data' in ai_learning_data.columns:\n",
        "    # Create a simple binary target variable (0 or 1)\n",
        "    # Replace this with your actual logic to define the target variable\n",
        "    ai_learning_data['target'] = ai_learning_data['data'].apply(lambda x: 1 if isinstance(x, str) and 'the' in x.lower() else 0)\n",
        "    y = ai_learning_data['target']\n",
        "else:\n",
        "    print(\"Error: 'ai_learning_data' DataFrame or 'data' column not found to create a hypothetical target.\")\n",
        "    y = np.random.randint(0, 2, size=tfidf_matrix.shape[0]) # Create a random target if data not available\n",
        "\n",
        "\n",
        "# Ensure tfidf_matrix is defined from the previous step\n",
        "if 'tfidf_matrix' in locals():\n",
        "    X = tfidf_matrix\n",
        "\n",
        "    # Split data into training and testing sets\n",
        "    # Using stratify=y helps maintain the proportion of the target variable in splits\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
        "\n",
        "    print(\"Data split into training and testing sets:\")\n",
        "    print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "    print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
        "\n",
        "    # Initialize and train a Logistic Regression model\n",
        "    model = LogisticRegression(max_iter=1000) # Increased max_iter for convergence\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    print(\"\\nLogistic Regression model trained.\")\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluate the model\n",
        "    print(\"\\nModel Evaluation (Classification Report):\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "else:\n",
        "    print(\"Error: 'tfidf_matrix' not found. Please run the TF-IDF generation cell first.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0164853c"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "# Ensure ai_learning_data is defined and has a column with the processed tokens\n",
        "if 'ai_learning_data' in locals() and isinstance(ai_learning_data, pd.DataFrame) and 'stemmed_tokens' in ai_learning_data.columns:\n",
        "    # Join the stemmed tokens back into strings for TF-IDF\n",
        "    # TF-IDF Vectorizer works on strings, not lists of tokens\n",
        "    ai_learning_data['stemmed_text'] = ai_learning_data['stemmed_tokens'].apply(lambda x: ' '.join(x) if isinstance(x, list) else '')\n",
        "\n",
        "    # Initialize TF-IDF Vectorizer\n",
        "    # You might want to adjust parameters like max_features, min_df, max_df, etc.\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "    # Fit and transform the stemmed text data\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(ai_learning_data['stemmed_text'])\n",
        "\n",
        "    print(\"TF-IDF features generated:\")\n",
        "    print(tfidf_matrix.shape) # Print the shape of the resulting matrix\n",
        "    # print(\"\\nFeature Names:\")\n",
        "    # print(tfidf_vectorizer.get_feature_names_out()) # Uncomment to see the vocabulary\n",
        "else:\n",
        "    print(\"Error: 'ai_learning_data' DataFrame or 'stemmed_tokens' column not found. Please run the preceding data preparation cells.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a941288"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer # Import PorterStemmer\n",
        "\n",
        "# Download necessary NLTK data (if not already downloaded)\n",
        "# nltk.download('punkt') # Moved to a separate cell for installation\n",
        "# nltk.download('stopwords') # Moved to a separate cell for installation\n",
        "# If you want to use lemmatization, you'll need 'wordnet':\n",
        "# nltk.download('wordnet')\n",
        "# from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def clean_text_for_ai(data):\n",
        "    \"\"\"Cleans text data for AI model training.\"\"\"\n",
        "    if isinstance(data, str):\n",
        "        # Remove HTML tags (a basic approach)\n",
        "        cleaned_data = re.sub('<.*?>', '', cleaned_data) # Corrected variable name\n",
        "        # Remove special characters and punctuation (keeping alphanumeric and spaces)\n",
        "        cleaned_data = re.sub(r'[^a-zA-Z0-9\\s]', '', cleaned_data)\n",
        "        # Convert to lowercase\n",
        "        cleaned_data = cleaned_data.lower()\n",
        "        return cleaned_data\n",
        "    return data # Return non-string data as is\n",
        "\n",
        "def tokenize_and_remove_stopwords(text):\n",
        "    \"\"\"Tokenizes text and removes stop words.\"\"\"\n",
        "    if isinstance(text, str):\n",
        "        tokens = word_tokenize(text)\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "        return filtered_tokens\n",
        "    return text # Return non-string data as is\n",
        "\n",
        "def stem_tokens(tokens):\n",
        "    \"\"\"Applies stemming to a list of tokens.\"\"\"\n",
        "    if isinstance(tokens, list):\n",
        "        stemmer = PorterStemmer()\n",
        "        stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
        "        return stemmed_tokens\n",
        "    return tokens # Return non-list data as is\n",
        "\n",
        "# If you want to use lemmatization instead:\n",
        "# def lemmatize_tokens(tokens):\n",
        "#     \"\"\"Applies lemmatization to a list of tokens.\"\"\"\n",
        "#     if isinstance(tokens, list):\n",
        "#         lemmatizer = WordNetLemmatizer()\n",
        "#         lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "#         return lemmatized_tokens\n",
        "#     return tokens\n",
        "\n",
        "\n",
        "# Apply the cleaning function to the 'data' column for 'text' data types\n",
        "# Ensure ai_learning_data is defined and has a 'data' and 'data_type' column\n",
        "if 'ai_learning_data' in locals() and isinstance(ai_learning_data, pd.DataFrame):\n",
        "    ai_learning_data['processed_data'] = ai_learning_data.apply(\n",
        "        lambda row: clean_text_for_ai(row['data']) if row['data_type'] == 'text' else row['data'],\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    print(\"Data after basic cleaning for text types:\")\n",
        "    display(ai_learning_data)\n",
        "\n",
        "    # Apply tokenization and stop word removal\n",
        "    ai_learning_data['tokens'] = ai_learning_data['processed_data'].apply(\n",
        "        lambda x: tokenize_and_remove_stopwords(x) if isinstance(x, str) else x\n",
        "    )\n",
        "\n",
        "    print(\"\\nData after tokenization and stop word removal:\")\n",
        "    display(ai_learning_data)\n",
        "\n",
        "    # Apply stemming\n",
        "    ai_learning_data['stemmed_tokens'] = ai_learning_data['tokens'].apply(\n",
        "        lambda x: stem_tokens(x) if isinstance(x, list) else x\n",
        "    )\n",
        "\n",
        "    print(\"\\nData after stemming:\")\n",
        "    display(ai_learning_data)\n",
        "\n",
        "    # If you want to use lemmatization instead, replace the stemming lines with this:\n",
        "    # ai_learning_data['lemmatized_tokens'] = ai_learning_data['tokens'].apply(\n",
        "    #     lambda x: lemmatize_tokens(x) if isinstance(x, list) else x\n",
        "    # )\n",
        "    # print(\"\\nData after lemmatization:\")\n",
        "    # display(ai_learning_data)\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"Error: ai_learning_data DataFrame not found. Please run the preceding cell to create it.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "from google.colab import output\n",
        "\n",
        "display(IPython.display.Javascript('''\n",
        "  window.someValue = new Promise(resolve => {\n",
        "    setTimeout(() => {\n",
        "      resolve(\"hello world!\");\n",
        "    }, 100);\n",
        "  });\n",
        "'''))\n",
        "\n",
        "\n",
        "value = output.eval_js('someValue');\n",
        "value"
      ],
      "metadata": {
        "id": "bU1MtLgyK_B5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0b88c674"
      },
      "source": [
        "%pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title AI prompt cell\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, Markdown,clear_output\n",
        "from google.colab import ai\n",
        "\n",
        "dropdown = widgets.Dropdown(\n",
        "    options=[],\n",
        "    layout={'width': 'auto'}\n",
        ")\n",
        "\n",
        "def update_model_list(new_options):\n",
        "    dropdown.options = new_options\n",
        "update_model_list(ai.list_models())\n",
        "\n",
        "text_input = widgets.Textarea(\n",
        "    placeholder='Ask me anything....',\n",
        "    layout={'width': 'auto', 'height': '100px'},\n",
        ")\n",
        "\n",
        "button = widgets.Button(\n",
        "    description='Submit Text',\n",
        "    disabled=False,\n",
        "    tooltip='Click to submit the text',\n",
        "    icon='check'\n",
        ")\n",
        "\n",
        "output_area = widgets.Output(\n",
        "     layout={'width': 'auto', 'max_height': '300px','overflow_y': 'scroll'}\n",
        ")\n",
        "\n",
        "def on_button_clicked(b):\n",
        "    with output_area:\n",
        "        output_area.clear_output(wait=False)\n",
        "        accumulated_content = \"\"\n",
        "        for new_chunk in ai.generate_text(prompt=text_input.value, model_name=dropdown.value, stream=True):\n",
        "            if new_chunk is None:\n",
        "                continue\n",
        "            accumulated_content += new_chunk\n",
        "            clear_output(wait=True)\n",
        "            display(Markdown(accumulated_content))\n",
        "\n",
        "button.on_click(on_button_clicked)\n",
        "vbox = widgets.GridBox([dropdown, text_input, button, output_area])\n",
        "\n",
        "display(HTML(\"\"\"\n",
        "<style>\n",
        ".widget-dropdown select {\n",
        "    font-size: 18px;\n",
        "    font-family: \"Arial\", sans-serif;\n",
        "}\n",
        ".widget-textarea textarea {\n",
        "    font-size: 18px;\n",
        "    font-family: \"Arial\", sans-serif;\n",
        "}\n",
        "</style>\n",
        "\"\"\"))\n",
        "display(vbox)\n"
      ],
      "metadata": {
        "id": "MapQz_JVJBgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62c7d83f"
      },
      "source": [
        "# Check loaded dataframe for missing values\n",
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d63e2a40"
      },
      "source": [
        "# Check the first few rows of the dataframe\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5b6b217"
      },
      "source": [
        "# Re-execute the cell to describe the dataframe\n",
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eb0a11c"
      },
      "source": [
        "# Re-execute the cell to load data from BigQuery\n",
        "import pandas as pd\n",
        "\n",
        "# https://cloud.google.com/resource-manager/docs/creating-managing-projects\n",
        "project_id = 'REPLACE WITH YOUR GOOGLE CLOUD PROJECT ID' # REPLACE WITH YOUR GOOGLE CLOUD PROJECT ID\n",
        "sample_count = 2000\n",
        "\n",
        "row_count = pd.io.gbq.read_gbq('''\n",
        "  SELECT\n",
        "    COUNT(*) as total\n",
        "  FROM `bigquery-public-data.samples.gsod`\n",
        "''', project_id=project_id).total[0]\n",
        "\n",
        "df = pd.io.gbq.read_gbq(f'''\n",
        "  SELECT\n",
        "    *\n",
        "  FROM\n",
        "    `bigquery-public-data.samples.gsod`\n",
        "  WHERE RAND() < {sample_count}/{row_count}\n",
        "''', project_id=project_id)\n",
        "\n",
        "print(f'Full dataset has {row_count} rows')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add `%load_ext cuml.accel` before importing sklearn to speed up operations using GPU\n"
      ],
      "metadata": {
        "id": "6HglTfk8JIqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %load_ext cuml.accel # Removed to avoid CUDA initialization error\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report # Import classification_report\n",
        "\n",
        "X, y = make_classification(n_samples=1000000, n_features=200, random_state=0)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "clf = LogisticRegression()\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "PGuDwHMeJIqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ee\n",
        "import geemap\n",
        "ee.Initialize()\n",
        "m = geemap.Map()\n",
        "\n",
        "# Load a Landsat 8 image\n",
        "image = (ee.ImageCollection('LANDSAT/LC08/C02/T1_L2')\n",
        "              .filter(ee.Filter.greaterThan('CLOUD_COVER', 30))\n",
        "              .filter(ee.Filter.lessThan('CLOUD_COVER', 60)).first())\n",
        "\n",
        "# Define a function to mask clouds using the QA_PIXEL band\n",
        "def maskL8sr(image):\n",
        "  # Bit 2 is cirrus, bit 3 is cloud , bit 4 is cloud shadow\n",
        "  cloudShadowBitMask = (1 << 2) | (1 << 3) | (1 << 4)\n",
        "  # Get the pixel QA band\n",
        "  qa = image.select('QA_PIXEL')\n",
        "  # Both flags should be set to zero, indicating clear conditions.\n",
        "  mask = qa.bitwiseAnd(cloudShadowBitMask).eq(0)\n",
        "  return image.updateMask(mask)\n",
        "\n",
        "# Apply the cloud mask\n",
        "masked_image = maskL8sr(image)\n",
        "\n",
        "# Define visualization parameters\n",
        "vis_params = {\n",
        "  'bands': ['SR_B4', 'SR_B3', 'SR_B2'],\n",
        "  'min': 0,\n",
        "  'max': 50000\n",
        "}\n",
        "\n",
        "# Center the map on the image\n",
        "m.centerObject(image, 7)\n",
        "\n",
        "# Add both original and masked images to the map\n",
        "m.add_layer(image, vis_params, 'Original Image')\n",
        "m.add_layer(masked_image, vis_params, 'Masked Image')\n",
        "m"
      ],
      "metadata": {
        "id": "D3tREDdSJx_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ee\n",
        "import geemap\n",
        "ee.Initialize()\n",
        "m = geemap.Map()\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define a point of interest in California (example: San Francisco)\n",
        "point = ee.Geometry.Point([-122.4194, 37.7749])\n",
        "\n",
        "# Load CHIRPS precipitation data\n",
        "chirps = ee.ImageCollection('UCSB-CHG/CHIRPS/PENTAD')\n",
        "\n",
        "# Filter data for a specific time period (example: 2022)\n",
        "chirps_filtered = chirps.filterDate('2022-01-01', '2022-12-31')\n",
        "\n",
        "# Extract time series data at the point using the 5000m scale\n",
        "time_series = ee.data.computeValue(chirps_filtered.getRegion(point, 5000))\n",
        "\n",
        "# Convert to pandas DataFrame\n",
        "df = pd.DataFrame(time_series[1:], columns=time_series[0])\n",
        "\n",
        "# Convert date column to datetime objects\n",
        "df['datetime'] = pd.to_datetime(df['time'], unit='ms')\n",
        "\n",
        "# Set datetime as index\n",
        "df = df.set_index('datetime')\n",
        "\n",
        "# Select the precipitation band ('precipitation')\n",
        "precipitation = df['precipitation']\n",
        "\n",
        "# Plot the time series using matplotlib\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(precipitation)\n",
        "plt.title('CHIRPS Precipitation Time Series (San Francisco, 2022)')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Precipitation (mm/pentad)')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R8k9jCvQLgdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ee\n",
        "import geemap\n",
        "ee.Initialize()\n",
        "m = geemap.Map()\n",
        "\n",
        "# Define coordinates for Paris\n",
        "x = 2.3522\n",
        "y = 48.8566\n",
        "\n",
        "# Create a point geometry for Paris\n",
        "paris_point = ee.Geometry.Point([x, y])\n",
        "\n",
        "# Create a 10km buffer around Paris\n",
        "buffer = paris_point.buffer(10000)\n",
        "\n",
        "# Change the dates as necessary.\n",
        "start_date = '2023-07-01'\n",
        "end_date = '2023-07-15'\n",
        "\n",
        "# Load a Sentinel-2 composite. It's also possible to use median() or other ways\n",
        "# to produce a composite.\n",
        "image = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n",
        "              .filterDate(start_date, end_date)\n",
        "              .mosaic())\n",
        "\n",
        "# Calculate NDVI\n",
        "ndvi = image.normalizedDifference(['B8', 'B4'])\n",
        "\n",
        "# Calculate mean NDVI within the buffer\n",
        "mean_ndvi = ndvi.reduceRegion(\n",
        "  reducer=ee.Reducer.mean(),\n",
        "  geometry=buffer,\n",
        "  scale=100 # Scale in meters, might need to be changed.\n",
        ")\n",
        "\n",
        "# Print the mean NDVI\n",
        "print('Mean NDVI:', mean_ndvi.get('nd').getInfo())\n",
        "\n",
        "# Display the buffer and NDVI on the map\n",
        "m.set_center(x, y, 10)\n",
        "m.add_layer(buffer, {}, 'Buffer')\n",
        "m.add_layer(\n",
        "    ndvi,\n",
        "    {'min': -1, 'max': 1, 'palette': ['red', 'white', 'green']}, 'NDVI')\n",
        "m"
      ],
      "metadata": {
        "id": "Ye6oAYwCKO6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48c91d3e"
      },
      "source": [
        "import pandas as pd\n",
        "import datetime\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Use a pandas DataFrame for more structured data handling\n",
        "# In a real application, this would likely be a database\n",
        "ai_learning_data = pd.DataFrame(columns=['timestamp', 'source', 'data', 'data_type'])\n",
        "\n",
        "def capture_learning_data(source, data, data_type='text'):\n",
        "    \"\"\"Captures structured data for AI learning.\"\"\"\n",
        "    global ai_learning_data\n",
        "    new_entry = pd.DataFrame([{\n",
        "        'timestamp': datetime.datetime.now(),\n",
        "        'source': source,\n",
        "        'data': data,\n",
        "        'data_type': data_type\n",
        "    }])\n",
        "    ai_learning_data = pd.concat([ai_learning_data, new_entry], ignore_index=True)\n",
        "    print(f\"Captured data from {source}: {data[:50]}...\") # Print a snippet\n",
        "\n",
        "# --- Example of capturing data (simulating different sources) ---\n",
        "\n",
        "# Simulate capturing data from a user\n",
        "capture_learning_data(source='user_123', data='The sky is blue today.')\n",
        "capture_learning_data(source='user_123', data='What is the capital of France?')\n",
        "\n",
        "# Simulate capturing data from an AI's output\n",
        "capture_learning_data(source='ai_456', data='Paris is the capital of France.', data_type='ai_output')\n",
        "\n",
        "# Simulate capturing data from a web source (hypothetical)\n",
        "capture_learning_data(source='web_scraper', data='<article>... content of a news article ...</article>', data_type='html')\n",
        "\n",
        "# --- Widget for capturing user input (similar to previous example, but using the new function) ---\n",
        "\n",
        "def on_submit_learning_input_clicked(b):\n",
        "    \"\"\"Handles the submission of user input for learning.\"\"\"\n",
        "    with learning_input_output:\n",
        "        clear_output(wait=True)\n",
        "        user_input = learning_input_textarea.value\n",
        "        if user_input:\n",
        "            capture_learning_data(source='user_input_widget', data=user_input, data_type='text')\n",
        "            print(f\"Submitted user input for learning: {user_input[:50]}...\")\n",
        "            learning_input_textarea.value = \"\" # Clear the textarea\n",
        "            display_learning_data() # Display the updated data\n",
        "        else:\n",
        "            print(\"Please enter some input.\")\n",
        "\n",
        "def display_learning_data():\n",
        "    \"\"\"Displays the current learning data DataFrame.\"\"\"\n",
        "    with learning_input_output:\n",
        "        print(\"\\n--- Current AI Learning Data ---\")\n",
        "        display(ai_learning_data)\n",
        "        print(\"--------------------------------\")\n",
        "\n",
        "\n",
        "learning_input_textarea = widgets.Textarea(\n",
        "    placeholder='Enter data for AI learning...',\n",
        "    layout={'width': 'auto', 'height': '100px'},\n",
        ")\n",
        "\n",
        "submit_learning_input_button = widgets.Button(\n",
        "    description='Submit for AI Learning',\n",
        "    disabled=False,\n",
        "    tooltip='Click to submit data for AI learning',\n",
        "    icon='send'\n",
        ")\n",
        "\n",
        "learning_input_output = widgets.Output()\n",
        "\n",
        "submit_learning_input_button.on_click(on_submit_learning_input_clicked)\n",
        "\n",
        "learning_input_vbox = widgets.VBox([learning_input_textarea, submit_learning_input_button, learning_input_output])\n",
        "\n",
        "print(\"Use the text area below to provide input to be captured for AI learning:\")\n",
        "display(learning_input_vbox)\n",
        "\n",
        "# Display the initial state of the learning data\n",
        "display_learning_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1a3f251f"
      },
      "source": [
        "# Display the head of df1\n",
        "print(\"Head of df1:\")\n",
        "display(df1.head())\n",
        "\n",
        "# Display information about df1\n",
        "print(\"\\nInfo of df1:\")\n",
        "df1.info()\n",
        "\n",
        "# Display the head of df2\n",
        "print(\"\\nHead of df2:\")\n",
        "display(df2.head())\n",
        "\n",
        "# Display information about df2\n",
        "print(\"\\nInfo of df2:\")\n",
        "df2.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99965e82"
      },
      "source": [
        "# Display the head of ai_learning_data\n",
        "print(\"Head of ai_learning_data:\")\n",
        "display(ai_learning_data.head())\n",
        "\n",
        "# Display information about ai_learning_data\n",
        "print(\"\\nInfo of ai_learning_data:\")\n",
        "ai_learning_data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('secretName')"
      ],
      "metadata": {
        "id": "MdTc54n9OUmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afb529d0"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Ensure ai_learning_data is defined\n",
        "if 'ai_learning_data' in locals() and isinstance(ai_learning_data, pd.DataFrame):\n",
        "    # Count the occurrences of each data type\n",
        "    data_type_counts = ai_learning_data['data_type'].value_counts()\n",
        "\n",
        "    # Create a bar plot\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    data_type_counts.plot(kind='bar')\n",
        "    plt.title('Distribution of Data Types in ai_learning_data')\n",
        "    plt.xlabel('Data Type')\n",
        "    plt.ylabel('Count')\n",
        "    plt.xticks(rotation=45, ha='right') # Rotate labels for better readability\n",
        "    plt.tight_layout() # Adjust layout to prevent labels overlapping\n",
        "    plt.show()\n",
        "\n",
        "    # You could also visualize the source\n",
        "    # source_counts = ai_learning_data['source'].value_counts()\n",
        "    # plt.figure(figsize=(8, 6))\n",
        "    # source_counts.plot(kind='bar')\n",
        "    # plt.title('Distribution of Data Sources in ai_learning_data')\n",
        "    # plt.xlabel('Source')\n",
        "    # plt.ylabel('Count')\n",
        "    # plt.xticks(rotation=45, ha='right')\n",
        "    # plt.tight_layout()\n",
        "    # plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"Error: 'ai_learning_data' DataFrame not found. Please run the cell that creates it.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3f03a15"
      },
      "source": [
        "import time\n",
        "import random\n",
        "\n",
        "class AIDeepThinkProcess:\n",
        "    \"\"\"Simulates an AI deep think process.\"\"\"\n",
        "    def __init__(self, ai_id):\n",
        "        self.ai_id = ai_id\n",
        "        self.idea = None\n",
        "        self.is_thinking = False\n",
        "\n",
        "    def start_thinking(self, query):\n",
        "        \"\"\"Simulates the AI starting a deep think process.\"\"\"\n",
        "        self.is_thinking = True\n",
        "        print(f\"AI {self.ai_id} is thinking about: {query}\")\n",
        "        # Simulate thinking time\n",
        "        time.sleep(random.uniform(1, 3))\n",
        "        self.idea = f\"Idea from AI {self.ai_id} on '{query}'\"\n",
        "        self.is_thinking = False\n",
        "        print(f\"AI {self.ai_id} finished thinking. Idea: {self.idea}\")\n",
        "        return self.idea\n",
        "\n",
        "class AIMultiCoordination:\n",
        "    \"\"\"Manages and coordinates multiple AI deep think processes.\"\"\"\n",
        "    def __init__(self, num_ais):\n",
        "        self.ais = [AIDeepThinkProcess(i + 1) for i in range(num_ais)]\n",
        "\n",
        "    def process_query_with_ais(self, query):\n",
        "        \"\"\"Sends a query to all AIs and collects their ideas.\"\"\"\n",
        "        print(f\"\\nCoordinating AIs for query: {query}\")\n",
        "        ideas = []\n",
        "        for ai_process in self.ais:\n",
        "            idea = ai_process.start_thinking(query)\n",
        "            ideas.append(idea)\n",
        "        print(\"\\nAll AIs have contributed their ideas.\")\n",
        "        return ideas\n",
        "\n",
        "# Example Usage:\n",
        "num_ais_to_coordinate = 3\n",
        "coordinator = AIMultiCoordination(num_ais_to_coordinate)\n",
        "user_query = \"How can AI and Blockchain be integrated?\"\n",
        "all_ideas = coordinator.process_query_with_ais(user_query)\n",
        "\n",
        "print(\"\\nCollected Ideas:\")\n",
        "for idea in all_ideas:\n",
        "    print(f\"- {idea}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "WqEZ0gT3I2Gp"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c54f0744"
      },
      "source": [
        "npm start"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80fe8b6b"
      },
      "source": [
        "{\n",
        "  \"ai\": {\n",
        "    \"deepThinkTimeout\": 5000,\n",
        "    \"maxAIs\": 10\n",
        "  },\n",
        "  \"blockchain\": {\n",
        "    \"blockTime\": 1000\n",
        "  },\n",
        "  \"algebra\": {\n",
        "    \"phi\": 1.618033988749895,\n",
        "    \"dimensions\": 3\n",
        "  }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cff35ba"
      },
      "source": [
        "npm test"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}